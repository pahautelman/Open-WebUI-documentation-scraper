reference: https://docs.openwebui.com/features/plugin/functions/filter
title: ðŸª„ Filter Function: Modify Inputs and Outputs

â­ FeaturesðŸ› ï¸ Tools & FunctionsðŸ§° FunctionsðŸª„ Filter Function

â­ Features

ðŸ› ï¸ Tools & Functions

ðŸ§° Functions

ðŸª„ Filter Function

Welcome to the comprehensive guide on Filter Functions in Open WebUI! Filters are a flexible and powerful plugin system for modifying data before it's sent to the Large Language Model (LLM) (input) or after itâ€™s returned from the LLM (output). Whether youâ€™re transforming inputs for better context or cleaning up outputs for improved readability, Filter Functions let you do it all.

This guide will break down what Filters are, how they work, their structure, and everything you need to know to build powerful and user-friendly filters of your own. Letâ€™s dig in, and donâ€™t worryâ€”Iâ€™ll use metaphors, examples, and tips to make everything crystal clear! ðŸŒŸ

ðŸŒŠ What Are Filters in Open WebUI?â€‹

Imagine Open WebUI as a stream of water flowing through pipes:

User inputs and LLM outputs are the water. Filters are the water treatment stages that clean, modify, and adapt the water before it reaches the final destination.

User inputs and LLM outputs are the water.

Filters are the water treatment stages that clean, modify, and adapt the water before it reaches the final destination.

Filters sit in the middle of the flowâ€”like checkpointsâ€”where you decide what needs to be adjusted.

Hereâ€™s a quick summary of what Filters do:

Modify User Inputs (Inlet Function): Tweak the input data before it reaches the AI model. This is where you enhance clarity, add context, sanitize text, or reformat messages to match specific requirements. Intercept Model Outputs (Stream Function): Capture and adjust the AIâ€™s responses as theyâ€™re generated by the model. This is useful for real-time modifications, like filtering out sensitive information or formatting the output for better readability. Modify Model Outputs (Outlet Function): Adjust the AI's response after itâ€™s processed, before showing it to the user. This can help refine, log, or adapt the data for a cleaner user experience.

Modify User Inputs (Inlet Function): Tweak the input data before it reaches the AI model. This is where you enhance clarity, add context, sanitize text, or reformat messages to match specific requirements.

Intercept Model Outputs (Stream Function): Capture and adjust the AIâ€™s responses as theyâ€™re generated by the model. This is useful for real-time modifications, like filtering out sensitive information or formatting the output for better readability.

Modify Model Outputs (Outlet Function): Adjust the AI's response after itâ€™s processed, before showing it to the user. This can help refine, log, or adapt the data for a cleaner user experience.

Key Concept: Filters are not standalone models but tools that enhance or transform the data traveling to and from models.

Filters are like translators or editors in the AI workflow: you can intercept and change the conversation without interrupting the flow.

ðŸ—ºï¸ Structure of a Filter Function: The Skeletonâ€‹

Let's start with the simplest representation of a Filter Function. Don't worry if some parts feel technical at firstâ€”weâ€™ll break it all down step by step!

ðŸ¦´ Basic Skeleton of a Filterâ€‹

from pydantic import BaseModelfrom typing import Optionalclass Filter: # Valves: Configuration options for the filter class Valves(BaseModel): pass def __init__(self): # Initialize valves (optional configuration for the Filter) self.valves = self.Valves() def inlet(self, body: dict) -> dict: # This is where you manipulate user inputs. print(f"inlet called: {body}") return body def stream(self, event: dict) -> dict: # This is where you modify streamed chunks of model output. print(f"stream event: {event}") return event def outlet(self, body: dict) -> None: # This is where you manipulate model outputs. print(f"outlet called: {body}")

from pydantic import BaseModelfrom typing import Optionalclass Filter: # Valves: Configuration options for the filter class Valves(BaseModel): pass def __init__(self): # Initialize valves (optional configuration for the Filter) self.valves = self.Valves() def inlet(self, body: dict) -> dict: # This is where you manipulate user inputs. print(f"inlet called: {body}") return body def stream(self, event: dict) -> dict: # This is where you modify streamed chunks of model output. print(f"stream event: {event}") return event def outlet(self, body: dict) -> None: # This is where you manipulate model outputs. print(f"outlet called: {body}")

ðŸŽ¯ Key Components Explainedâ€‹

1ï¸âƒ£ Valves Class (Optional Settings)â€‹

Valves

Think of Valves as the knobs and sliders for your filter. If you want to give users configurable options to adjust your Filterâ€™s behavior, you define those here.

class Valves(BaseModel): OPTION_NAME: str = "Default Value"

class Valves(BaseModel): OPTION_NAME: str = "Default Value"

For example: If you're creating a filter that converts responses into uppercase, you might allow users to configure whether every output gets totally capitalized via a valve like TRANSFORM_UPPERCASE: bool = True/False.

TRANSFORM_UPPERCASE: bool = True/False

2ï¸âƒ£ inlet Function (Input Pre-Processing)â€‹

inlet

The inlet function is like prepping food before cooking. Imagine youâ€™re a chef: before the ingredients go into the recipe (the LLM in this case), you might wash vegetables, chop onions, or season the meat. Without this step, your final dish could lack flavor, have unwashed produce, or simply be inconsistent.

inlet

In the world of Open WebUI, the inlet function does this important prep work on the user input before itâ€™s sent to the model. It ensures the input is as clean, contextual, and helpful as possible for the AI to handle.

inlet

ðŸ“¥ Input:

body: The raw input from Open WebUI to the model. It is in the format of a chat-completion request (usually a dictionary that includes fields like the conversation's messages, model settings, and other metadata). Think of this as your recipe ingredients.

body: The raw input from Open WebUI to the model. It is in the format of a chat-completion request (usually a dictionary that includes fields like the conversation's messages, model settings, and other metadata). Think of this as your recipe ingredients.

body

ðŸš€ Your Task: Modify and return the body. The modified version of the body is what the LLM works with, so this is your chance to bring clarity, structure, and context to the input.

body

body

ðŸ³ Why Would You Use the inlet?â€‹

inlet

Adding Context: Automatically append crucial information to the userâ€™s input, especially if their text is vague or incomplete. For example, you might add "You are a friendly assistant" or "Help this user troubleshoot a software bug." Formatting Data: If the input requires a specific format, like JSON or Markdown, you can transform it before sending it to the model. Sanitizing Input: Remove unwanted characters, strip potentially harmful or confusing symbols (like excessive whitespace or emojis), or replace sensitive information. Streamlining User Input: If your modelâ€™s output improves with additional guidance, you can use the inlet to inject clarifying instructions automatically!

Adding Context: Automatically append crucial information to the userâ€™s input, especially if their text is vague or incomplete. For example, you might add "You are a friendly assistant" or "Help this user troubleshoot a software bug."

Adding Context: Automatically append crucial information to the userâ€™s input, especially if their text is vague or incomplete. For example, you might add "You are a friendly assistant" or "Help this user troubleshoot a software bug."

Formatting Data: If the input requires a specific format, like JSON or Markdown, you can transform it before sending it to the model.

Formatting Data: If the input requires a specific format, like JSON or Markdown, you can transform it before sending it to the model.

Sanitizing Input: Remove unwanted characters, strip potentially harmful or confusing symbols (like excessive whitespace or emojis), or replace sensitive information.

Sanitizing Input: Remove unwanted characters, strip potentially harmful or confusing symbols (like excessive whitespace or emojis), or replace sensitive information.

Streamlining User Input: If your modelâ€™s output improves with additional guidance, you can use the inlet to inject clarifying instructions automatically!

Streamlining User Input: If your modelâ€™s output improves with additional guidance, you can use the inlet to inject clarifying instructions automatically!

inlet

ðŸ’¡ Example Use Cases: Build on Food Prepâ€‹

ðŸ¥— Example 1: Adding System Contextâ€‹

Letâ€™s say the LLM is a chef preparing a dish for Italian cuisine, but the user hasnâ€™t mentioned "This is for Italian cooking." You can ensure the message is clear by appending this context before sending the data to the model.

def inlet(self, body: dict, __user__: Optional[dict] = None) -> dict: # Add system message for Italian context in the conversation context_message = { "role": "system", "content": "You are helping the user prepare an Italian meal." } # Insert the context at the beginning of the chat history body.setdefault("messages", []).insert(0, context_message) return body

def inlet(self, body: dict, __user__: Optional[dict] = None) -> dict: # Add system message for Italian context in the conversation context_message = { "role": "system", "content": "You are helping the user prepare an Italian meal." } # Insert the context at the beginning of the chat history body.setdefault("messages", []).insert(0, context_message) return body

ðŸ“– What Happens?

Any user input like "What are some good dinner ideas?" now carries the Italian theme because weâ€™ve set the system context! Cheesecake might not show up as an answer, but pasta sure will.

Any user input like "What are some good dinner ideas?" now carries the Italian theme because weâ€™ve set the system context! Cheesecake might not show up as an answer, but pasta sure will.

ðŸ”ª Example 2: Cleaning Input (Remove Odd Characters)â€‹

Suppose the input from the user looks messy or includes unwanted symbols like !!!, making the conversation inefficient or harder for the model to parse. You can clean it up while preserving the core content.

!!!

def inlet(self, body: dict, __user__: Optional[dict] = None) -> dict: # Clean the last user input (from the end of the 'messages' list) last_message = body["messages"][-1]["content"] body["messages"][-1]["content"] = last_message.replace("!!!", "").strip() return body

def inlet(self, body: dict, __user__: Optional[dict] = None) -> dict: # Clean the last user input (from the end of the 'messages' list) last_message = body["messages"][-1]["content"] body["messages"][-1]["content"] = last_message.replace("!!!", "").strip() return body

ðŸ“– What Happens?

Before: "How can I debug this issue!!!" âž¡ï¸ Sent to the model as "How can I debug this issue"

Before: "How can I debug this issue!!!" âž¡ï¸ Sent to the model as "How can I debug this issue"

"How can I debug this issue!!!"

"How can I debug this issue"

Note: The user feels the same, but the model processes a cleaner and easier-to-understand query.

ðŸ“Š How inlet Helps Optimize Input for the LLM:â€‹

inlet

Improves accuracy by clarifying ambiguous queries. Makes the AI more efficient by removing unnecessary noise like emojis, HTML tags, or extra punctuation. Ensures consistency by formatting user input to match the modelâ€™s expected patterns or schemas (like, say, JSON for a specific use case).

Improves accuracy by clarifying ambiguous queries.

Makes the AI more efficient by removing unnecessary noise like emojis, HTML tags, or extra punctuation.

Ensures consistency by formatting user input to match the modelâ€™s expected patterns or schemas (like, say, JSON for a specific use case).

ðŸ’­ Think of inlet as the sous-chef in your kitchenâ€”ensuring everything that goes into the model (your AI "recipe") has been prepped, cleaned, and seasoned to perfection. The better the input, the better the output!

inlet

ðŸ†• 3ï¸âƒ£ stream Hook (New in Open WebUI 0.5.17)â€‹

stream

ðŸ”„ What is the stream Hook?â€‹

stream

The stream function is a new feature introduced in Open WebUI 0.5.17 that allows you to intercept and modify streamed model responses in real time.

stream

Unlike outlet, which processes an entire completed response, stream operates on individual chunks as they are received from the model.

outlet

stream

ðŸ› ï¸ When to Use the Stream Hook?â€‹

Modify streaming responses before they are displayed to users. Implement real-time censorship or cleanup. Monitor streamed data for logging/debugging.

Modify streaming responses before they are displayed to users.

Implement real-time censorship or cleanup.

Monitor streamed data for logging/debugging.

ðŸ“œ Example: Logging Streaming Chunksâ€‹

Hereâ€™s how you can inspect and modify streamed LLM responses:

def stream(self, event: dict) -> dict: print(event) # Print each incoming chunk for inspection return event

def stream(self, event: dict) -> dict: print(event) # Print each incoming chunk for inspection return event

Example Streamed Events:

{'id': 'chatcmpl-B4l99MMaP3QLGU5uV7BaBM0eDS0jb','choices': [{'delta': {'content': 'Hi'}}]}{'id': 'chatcmpl-B4l99MMaP3QLGU5uV7BaBM0eDS0jb','choices': [{'delta': {'content': '!'}}]}{'id': 'chatcmpl-B4l99MMaP3QLGU5uV7BaBM0eDS0jb','choices': [{'delta': {'content': ' ðŸ˜Š'}}]}

{'id': 'chatcmpl-B4l99MMaP3QLGU5uV7BaBM0eDS0jb','choices': [{'delta': {'content': 'Hi'}}]}{'id': 'chatcmpl-B4l99MMaP3QLGU5uV7BaBM0eDS0jb','choices': [{'delta': {'content': '!'}}]}{'id': 'chatcmpl-B4l99MMaP3QLGU5uV7BaBM0eDS0jb','choices': [{'delta': {'content': ' ðŸ˜Š'}}]}

ðŸ“– What Happens?

Each line represents a small fragment of the model's streamed response. The delta.content field contains the progressively generated text.

Each line represents a small fragment of the model's streamed response.

The delta.content field contains the progressively generated text.

delta.content

ðŸ”„ Example: Filtering Out Emojis from Streamed Dataâ€‹

def stream(self, event: dict) -> dict: for choice in event.get("choices", []): delta = choice.get("delta", {}) if "content" in delta: delta["content"] = delta["content"].replace("ðŸ˜Š", "") # Strip emojis return event

def stream(self, event: dict) -> dict: for choice in event.get("choices", []): delta = choice.get("delta", {}) if "content" in delta: delta["content"] = delta["content"].replace("ðŸ˜Š", "") # Strip emojis return event

ðŸ“– Before: "Hi ðŸ˜Š" ðŸ“– After: "Hi"

"Hi ðŸ˜Š"

"Hi"

4ï¸âƒ£ outlet Function (Output Post-Processing)â€‹

outlet

The outlet function is like a proofreader: tidy up the AI's response (or make final changes) after itâ€™s processed by the LLM.

outlet

ðŸ“¤ Input:

body: This contains all current messages in the chat (user history + LLM replies).

body: This contains all current messages in the chat (user history + LLM replies).

body

ðŸš€ Your Task: Modify this body. You can clean, append, or log changes, but be mindful of how each adjustment impacts the user experience.

body

ðŸ’¡ Best Practices:

Prefer logging over direct edits in the outlet (e.g., for debugging or analytics). If heavy modifications are needed (like formatting outputs), consider using the pipe function instead.

Prefer logging over direct edits in the outlet (e.g., for debugging or analytics).

If heavy modifications are needed (like formatting outputs), consider using the pipe function instead.

ðŸ’¡ Example Use Case: Strip out sensitive API responses you don't want the user to see:

def outlet(self, body: dict, __user__: Optional[dict] = None) -> dict: for message in body["messages"]: message["content"] = message["content"].replace("<API_KEY>", "[REDACTED]") return body

def outlet(self, body: dict, __user__: Optional[dict] = None) -> dict: for message in body["messages"]: message["content"] = message["content"].replace("<API_KEY>", "[REDACTED]") return body

ðŸŒŸ Filters in Action: Building Practical Examplesâ€‹

Letâ€™s build some real-world examples to see how youâ€™d use Filters!

ðŸ“š Example #1: Add Context to Every User Inputâ€‹

Want the LLM to always know it's assisting a customer in troubleshooting software bugs? You can add instructions like "You're a software troubleshooting assistant" to every user query.

class Filter: def inlet(self, body: dict, __user__: Optional[dict] = None) -> dict: context_message = { "role": "system", "content": "You're a software troubleshooting assistant." } body.setdefault("messages", []).insert(0, context_message) return body

class Filter: def inlet(self, body: dict, __user__: Optional[dict] = None) -> dict: context_message = { "role": "system", "content": "You're a software troubleshooting assistant." } body.setdefault("messages", []).insert(0, context_message) return body

ðŸ“š Example #2: Highlight Outputs for Easy Readingâ€‹

Returning output in Markdown or another formatted style? Use the outlet function!

outlet

class Filter: def outlet(self, body: dict, __user__: Optional[dict] = None) -> dict: # Add "highlight" markdown for every response for message in body["messages"]: if message["role"] == "assistant": # Target model response message["content"] = f"**{message['content']}**" # Highlight with Markdown return body

class Filter: def outlet(self, body: dict, __user__: Optional[dict] = None) -> dict: # Add "highlight" markdown for every response for message in body["messages"]: if message["role"] == "assistant": # Target model response message["content"] = f"**{message['content']}**" # Highlight with Markdown return body

ðŸš§ Potential Confusion: Clear FAQ ðŸ›‘â€‹

Q: How Are Filters Different From Pipe Functions?â€‹

Filters modify data going to and coming from models but do not significantly interact with logic outside of these phases. Pipes, on the other hand:

Can integrate external APIs or significantly transform how the backend handles operations. Expose custom logic as entirely new "models."

Can integrate external APIs or significantly transform how the backend handles operations.

Expose custom logic as entirely new "models."

Q: Can I Do Heavy Post-Processing Inside outlet?â€‹

outlet

You can, but itâ€™s not the best practice.:

Filters are designed to make lightweight changes or apply logging. If heavy modifications are required, consider a Pipe Function instead.

Filters are designed to make lightweight changes or apply logging.

If heavy modifications are required, consider a Pipe Function instead.

ðŸŽ‰ Recap: Why Build Filter Functions?â€‹

By now, youâ€™ve learned:

Inlet manipulates user inputs (pre-processing). Stream intercepts and modifies streamed model outputs (real-time). Outlet tweaks AI outputs (post-processing). Filters are best for lightweight, real-time alterations to the data flow. With Valves, you empower users to configure Filters dynamically for tailored behavior.

Inlet manipulates user inputs (pre-processing).

Stream intercepts and modifies streamed model outputs (real-time).

Outlet tweaks AI outputs (post-processing).

Filters are best for lightweight, real-time alterations to the data flow.

With Valves, you empower users to configure Filters dynamically for tailored behavior.

ðŸš€ Your Turn: Start experimenting! What small tweak or context addition could elevate your Open WebUI experience? Filters are fun to build, flexible to use, and can take your models to the next level!

Happy coding! âœ¨