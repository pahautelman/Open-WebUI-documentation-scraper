reference: https://docs.openwebui.com/tutorials/integrations/deepseekr1-dynamic
title: üêã Run DeepSeek R1 Dynamic 1.58-bit with Llama.cpp

üìù Tutorialsüîó Integrationsüêã Run DeepSeek R1 Dynamic 1.58-bit with Llama.cpp

üìù Tutorials

üîó Integrations

üêã Run DeepSeek R1 Dynamic 1.58-bit with Llama.cpp

A huge shoutout to UnslothAI for their incredible efforts! Thanks to their hard work, we can now run the full DeepSeek-R1 671B parameter model in its dynamic 1.58-bit quantized form (compressed to just 131GB) on Llama.cpp! And the best part? You no longer have to despair about needing massive enterprise-class GPUs or servers ‚Äî it‚Äôs possible to run this model on your personal machine (albeit slowly for most consumer hardware).

The only true DeepSeek-R1 model on Ollama is the 671B version available here: https://ollama.com/library/deepseek-r1:671b. Other versions are distilled models.

This guide focuses on running the full DeepSeek-R1 Dynamic 1.58-bit quantized model using Llama.cpp integrated with Open WebUI. For this tutorial, we‚Äôll demonstrate the steps with an M4 Max + 128GB RAM machine. You can adapt the settings to your own configuration.

Step 1: Install Llama.cpp‚Äã

You can either:

Download the prebuilt binaries Or build it yourself: Follow the instructions here: Llama.cpp Build Guide

Download the prebuilt binaries

Or build it yourself: Follow the instructions here: Llama.cpp Build Guide

Step 2: Download the Model Provided by UnslothAI‚Äã

Head over to Unsloth‚Äôs Hugging Face page and download the appropriate dynamic quantized version of DeepSeek-R1. For this tutorial, we‚Äôll use the 1.58-bit (131GB) version, which is highly optimized yet remains surprisingly functional.

Know your "working directory" ‚Äî where your Python script or terminal session is running. The model files will download to a subfolder of that directory by default, so be sure you know its path! For example, if you're running the command below in /Users/yourname/Documents/projects, your downloaded model will be saved under /Users/yourname/Documents/projects/DeepSeek-R1-GGUF.

/Users/yourname/Documents/projects

/Users/yourname/Documents/projects/DeepSeek-R1-GGUF

To understand more about UnslothAI‚Äôs development process and why these dynamic quantized versions are so efficient, check out their blog post: UnslothAI DeepSeek R1 Dynamic Quantization.

Here‚Äôs how to download the model programmatically:

# Install Hugging Face dependencies before running this:# pip install huggingface_hub hf_transferfrom huggingface_hub import snapshot_downloadsnapshot_download( repo_id = "unsloth/DeepSeek-R1-GGUF", # Specify the Hugging Face repo local_dir = "DeepSeek-R1-GGUF", # Model will download into this directory allow_patterns = ["*UD-IQ1_S*"], # Only download the 1.58-bit version)

# Install Hugging Face dependencies before running this:# pip install huggingface_hub hf_transferfrom huggingface_hub import snapshot_downloadsnapshot_download( repo_id = "unsloth/DeepSeek-R1-GGUF", # Specify the Hugging Face repo local_dir = "DeepSeek-R1-GGUF", # Model will download into this directory allow_patterns = ["*UD-IQ1_S*"], # Only download the 1.58-bit version)

Once the download completes, you‚Äôll find the model files in a directory structure like this:

DeepSeek-R1-GGUF/‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S/‚îÇ ‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf‚îÇ ‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S-00002-of-00003.gguf‚îÇ ‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S-00003-of-00003.gguf

DeepSeek-R1-GGUF/‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S/‚îÇ ‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf‚îÇ ‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S-00002-of-00003.gguf‚îÇ ‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S-00003-of-00003.gguf

üõ†Ô∏è Update paths in the later steps to match your specific directory structure. For example, if your script was in /Users/tim/Downloads, the full path to the GGUF file would be: /Users/tim/Downloads/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf.

/Users/tim/Downloads

/Users/tim/Downloads/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf

Step 3: Make Sure Open WebUI is Installed and Running‚Äã

If you don‚Äôt already have Open WebUI installed, no worries! It‚Äôs a simple setup. Just follow the Open WebUI documentation here. Once installed, start the application ‚Äî we‚Äôll connect it in a later step to interact with the DeepSeek-R1 model.

Step 4: Serve the Model Using Llama.cpp‚Äã

Now that the model is downloaded, the next step is to run it using Llama.cpp‚Äôs server mode. Before you begin:

Locate the llama-server binary. If you built from source (as outlined in Step 1), the llama-server executable will be located in llama.cpp/build/bin. Navigate to this directory by using the cd command: cd [path-to-llama-cpp]/llama.cpp/build/bin Replace [path-to-llama-cpp] with the location where you cloned or built Llama.cpp. For example: cd ~/Documents/workspace/llama.cpp/build/bin Point to your model folder. Use the full path to the downloaded GGUF files created in Step 2. When serving the model, specify the first part of the split GGUF files (e.g., DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf).

Locate the llama-server binary. If you built from source (as outlined in Step 1), the llama-server executable will be located in llama.cpp/build/bin. Navigate to this directory by using the cd command: cd [path-to-llama-cpp]/llama.cpp/build/bin Replace [path-to-llama-cpp] with the location where you cloned or built Llama.cpp. For example: cd ~/Documents/workspace/llama.cpp/build/bin

Locate the llama-server binary. If you built from source (as outlined in Step 1), the llama-server executable will be located in llama.cpp/build/bin. Navigate to this directory by using the cd command:

llama-server

llama-server

llama.cpp/build/bin

cd

cd [path-to-llama-cpp]/llama.cpp/build/bin

cd [path-to-llama-cpp]/llama.cpp/build/bin

Replace [path-to-llama-cpp] with the location where you cloned or built Llama.cpp. For example:

[path-to-llama-cpp]

cd ~/Documents/workspace/llama.cpp/build/bin

cd ~/Documents/workspace/llama.cpp/build/bin

Point to your model folder. Use the full path to the downloaded GGUF files created in Step 2. When serving the model, specify the first part of the split GGUF files (e.g., DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf).

Point to your model folder. Use the full path to the downloaded GGUF files created in Step 2. When serving the model, specify the first part of the split GGUF files (e.g., DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf).

DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf

Here‚Äôs the command to start the server:

./llama-server \ --model /[your-directory]/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \ --port 10000 \ --ctx-size 1024 \ --n-gpu-layers 40

./llama-server \ --model /[your-directory]/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \ --port 10000 \ --ctx-size 1024 \ --n-gpu-layers 40

üîë Parameters to Customize Based on Your Machine:

--model: Replace /[your-directory]/ with the path where the GGUF files were downloaded in Step 2. --port: The server default is 8080, but feel free to change it based on your port availability. --ctx-size: Determines context length (number of tokens). You can increase it if your hardware allows, but be cautious of rising RAM/VRAM usage. --n-gpu-layers: Set the number of layers you want to offload to your GPU for faster inference. The exact number depends on your GPU‚Äôs memory capacity ‚Äî reference Unsloth‚Äôs table for specific recommendations.

--model: Replace /[your-directory]/ with the path where the GGUF files were downloaded in Step 2.

--model

/[your-directory]/

--port: The server default is 8080, but feel free to change it based on your port availability.

--port

8080

--ctx-size: Determines context length (number of tokens). You can increase it if your hardware allows, but be cautious of rising RAM/VRAM usage.

--ctx-size

--n-gpu-layers: Set the number of layers you want to offload to your GPU for faster inference. The exact number depends on your GPU‚Äôs memory capacity ‚Äî reference Unsloth‚Äôs table for specific recommendations.

--n-gpu-layers

For example, if your model was downloaded to /Users/tim/Documents/workspace, your command would look like this:

/Users/tim/Documents/workspace

./llama-server \ --model /Users/tim/Documents/workspace/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \ --port 10000 \ --ctx-size 1024 \ --n-gpu-layers 40

./llama-server \ --model /Users/tim/Documents/workspace/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \ --port 10000 \ --ctx-size 1024 \ --n-gpu-layers 40

Once the server starts, it will host a local OpenAI-compatible API endpoint at:

http://127.0.0.1:10000

http://127.0.0.1:10000

üñ•Ô∏è Llama.cpp Server Running

[Image: Server Screenshot]

After running the command, you should see a message confirming the server is active and listening on port 10000.

Be sure to keep this terminal session running, as it serves the model for all subsequent steps.

Step 5: Connect Llama.cpp to Open WebUI‚Äã

Go to Admin Settings in Open WebUI. Navigate to Connections > OpenAI Connections. Add the following details for the new connection: URL: http://127.0.0.1:10000/v1 (or http://host.docker.internal:10000/v1 when running Open WebUI in docker) API Key: none

Go to Admin Settings in Open WebUI.

Navigate to Connections > OpenAI Connections.

Add the following details for the new connection: URL: http://127.0.0.1:10000/v1 (or http://host.docker.internal:10000/v1 when running Open WebUI in docker) API Key: none

URL: http://127.0.0.1:10000/v1 (or http://host.docker.internal:10000/v1 when running Open WebUI in docker) API Key: none

URL: http://127.0.0.1:10000/v1 (or http://host.docker.internal:10000/v1 when running Open WebUI in docker)

http://127.0.0.1:10000/v1

http://host.docker.internal:10000/v1

API Key: none

none

üñ•Ô∏è Adding Connection in Open WebUI

[Image: Connection Screenshot]

After running the command, you should see a message confirming the server is active and listening on port 10000.

Once the connection is saved, you can start querying DeepSeek-R1 directly from Open WebUI! üéâ

Example: Generating Responses‚Äã

You can now use Open WebUI‚Äôs chat interface to interact with the DeepSeek-R1 Dynamic 1.58-bit model.

[Image: Response Screenshot]

Notes and Considerations‚Äã

Performance: Running a massive 131GB model like DeepSeek-R1 on personal hardware will be slow. Even with our M4 Max (128GB RAM), inference speeds were modest. But the fact that it works at all is a testament to UnslothAI‚Äôs optimizations. VRAM/Memory Requirements: Ensure sufficient VRAM and system RAM for optimal performance. With low-end GPUs or CPU-only setups, expect slower speeds (but it‚Äôs still doable!).

Performance: Running a massive 131GB model like DeepSeek-R1 on personal hardware will be slow. Even with our M4 Max (128GB RAM), inference speeds were modest. But the fact that it works at all is a testament to UnslothAI‚Äôs optimizations.

Performance: Running a massive 131GB model like DeepSeek-R1 on personal hardware will be slow. Even with our M4 Max (128GB RAM), inference speeds were modest. But the fact that it works at all is a testament to UnslothAI‚Äôs optimizations.

VRAM/Memory Requirements: Ensure sufficient VRAM and system RAM for optimal performance. With low-end GPUs or CPU-only setups, expect slower speeds (but it‚Äôs still doable!).

VRAM/Memory Requirements: Ensure sufficient VRAM and system RAM for optimal performance. With low-end GPUs or CPU-only setups, expect slower speeds (but it‚Äôs still doable!).

Thanks to UnslothAI and Llama.cpp, running one of the largest open-source reasoning models, DeepSeek-R1 (1.58-bit version), is finally accessible to individuals. While it‚Äôs challenging to run such models on consumer hardware, the ability to do so without massive computational infrastructure is a significant technological milestone.

‚≠ê Big thanks to the community for pushing the boundaries of open AI research.

Happy experimenting! üöÄ