reference: https://docs.openwebui.com/tutorials/tips/improve-performance-local
title: Improve Performance with Dedicated Task Models

ðŸ“ TutorialsðŸ’¡ Tips & Tricksâš¡ Improve Local LLM Performance with Dedicated Task Models

ðŸ“ Tutorials

ðŸ’¡ Tips & Tricks

âš¡ Improve Local LLM Performance with Dedicated Task Models

Open-WebUI provides several automated featuresâ€”such as title generation, tag creation, autocomplete, and search query generationâ€”to enhance the user experience. However, these features can generate multiple simultaneous requests to your local model, which may impact performance on systems with limited resources.

This guide explains how to optimize your setup by configuring a dedicated, lightweight task model or by selectively disabling automation features, ensuring that your primary chat functionality remains responsive and efficient.

[!TIP]

Why Does Open-WebUI Feel Slow?â€‹

By default, Open-WebUI has several background tasks that can make it feel like magic but can also place a heavy load on local resources:

Title Generation Tag Generation Autocomplete Generation (this function triggers on every keystroke) Search Query Generation

Title Generation

Tag Generation

Autocomplete Generation (this function triggers on every keystroke)

Search Query Generation

Each of these features makes asynchronous requests to your model. For example, continuous calls from the autocomplete feature can significantly delay responses on devices with limited memory >or processing power, such as a Mac with 32GB of RAM running a 32B quantized model.

Optimizing the task model can help isolate these background tasks from your main chat application, improving overall responsiveness.

âš¡ How to Optimize Task Model Performanceâ€‹

Follow these steps to configure an efficient task model:

Step 1: Access the Admin Panelâ€‹

Open Open-WebUI in your browser. Navigate to the Admin Panel. Click on Settings in the sidebar.

Open Open-WebUI in your browser.

Navigate to the Admin Panel.

Click on Settings in the sidebar.

Step 2: Configure the Task Modelâ€‹

Go to Interface > Set Task Model. Choose one of the following options based on your needs: Lightweight Local Model (Recommended) Select a compact model such as Llama 3.2 3B or Qwen2.5 3B. These models offer rapid responses while consuming minimal system resources. Hosted API Endpoint (For Maximum Speed) Connect to a hosted API service to handle task processing. This can be very cheap. For example, OpenRouter offers Llama and Qwen models at less than 1.5 cents per million input tokens. Disable Unnecessary Automation If certain automated features are not required, disable them to reduce extraneous background callsâ€”especially features like autocomplete.

Go to Interface > Set Task Model.

Go to Interface > Set Task Model.

Choose one of the following options based on your needs: Lightweight Local Model (Recommended) Select a compact model such as Llama 3.2 3B or Qwen2.5 3B. These models offer rapid responses while consuming minimal system resources. Hosted API Endpoint (For Maximum Speed) Connect to a hosted API service to handle task processing. This can be very cheap. For example, OpenRouter offers Llama and Qwen models at less than 1.5 cents per million input tokens. Disable Unnecessary Automation If certain automated features are not required, disable them to reduce extraneous background callsâ€”especially features like autocomplete.

Choose one of the following options based on your needs:

Lightweight Local Model (Recommended) Select a compact model such as Llama 3.2 3B or Qwen2.5 3B. These models offer rapid responses while consuming minimal system resources. Hosted API Endpoint (For Maximum Speed) Connect to a hosted API service to handle task processing. This can be very cheap. For example, OpenRouter offers Llama and Qwen models at less than 1.5 cents per million input tokens. Disable Unnecessary Automation If certain automated features are not required, disable them to reduce extraneous background callsâ€”especially features like autocomplete.

Lightweight Local Model (Recommended) Select a compact model such as Llama 3.2 3B or Qwen2.5 3B. These models offer rapid responses while consuming minimal system resources.

Lightweight Local Model (Recommended)

Select a compact model such as Llama 3.2 3B or Qwen2.5 3B. These models offer rapid responses while consuming minimal system resources.

Select a compact model such as Llama 3.2 3B or Qwen2.5 3B.

These models offer rapid responses while consuming minimal system resources.

Hosted API Endpoint (For Maximum Speed) Connect to a hosted API service to handle task processing. This can be very cheap. For example, OpenRouter offers Llama and Qwen models at less than 1.5 cents per million input tokens.

Hosted API Endpoint (For Maximum Speed)

Connect to a hosted API service to handle task processing. This can be very cheap. For example, OpenRouter offers Llama and Qwen models at less than 1.5 cents per million input tokens.

Connect to a hosted API service to handle task processing.

This can be very cheap. For example, OpenRouter offers Llama and Qwen models at less than 1.5 cents per million input tokens.

Disable Unnecessary Automation If certain automated features are not required, disable them to reduce extraneous background callsâ€”especially features like autocomplete.

Disable Unnecessary Automation

If certain automated features are not required, disable them to reduce extraneous background callsâ€”especially features like autocomplete.

If certain automated features are not required, disable them to reduce extraneous background callsâ€”especially features like autocomplete.

[Image: Local Model Configuration Set to Qwen2.5:3b]

Step 3: Save Your Changes and Testâ€‹

Save the new configuration. Interact with your chat interface and observe the responsiveness. If necessary, adjust by further disabling unused automation features or experimenting with different task models.

Save the new configuration.

Interact with your chat interface and observe the responsiveness.

If necessary, adjust by further disabling unused automation features or experimenting with different task models.

ðŸš€ Recommended Setup for Local Modelsâ€‹

Implementing these recommendations can greatly improve the responsiveness of Open-WebUI while allowing your local models to efficiently handle chat interactions.

ðŸ’¡ Additional Tipsâ€‹

Monitor System Resources: Use your operating systemâ€™s tools (such as Activity Monitor on macOS or Task Manager on Windows) to keep an eye on resource usage. Reduce Parallel Model Calls: Limiting background automation prevents simultaneous requests from overwhelming your LLM. Experiment with Configurations: Test different lightweight models or hosted endpoints to find the optimal balance between speed and functionality. Stay Updated: Regular updates to Open-WebUI often include performance improvements and bug fixes, so keep your software current.

Monitor System Resources: Use your operating systemâ€™s tools (such as Activity Monitor on macOS or Task Manager on Windows) to keep an eye on resource usage.

Reduce Parallel Model Calls: Limiting background automation prevents simultaneous requests from overwhelming your LLM.

Experiment with Configurations: Test different lightweight models or hosted endpoints to find the optimal balance between speed and functionality.

Stay Updated: Regular updates to Open-WebUI often include performance improvements and bug fixes, so keep your software current.

By applying these configuration changes, you'll support a more responsive and efficient Open-WebUI experience, allowing your local LLM to focus on delivering high-quality chat interactions without unnecessary delays.