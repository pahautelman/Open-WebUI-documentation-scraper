reference: https://docs.openwebui.com/troubleshooting/connection-error
title: ğŸš§ Server Connectivity Issues

ğŸ› ï¸ TroubleshootingğŸš§ Server Connectivity Issues

ğŸ› ï¸ Troubleshooting

ğŸš§ Server Connectivity Issues

We're here to help you get everything set up and running smoothly. Below, you'll find step-by-step instructions tailored for different scenarios to solve common connection issues with Ollama and external servers like Hugging Face.

ğŸŒŸ Connection to Ollama Serverâ€‹

ğŸš€ Accessing Ollama from Open WebUIâ€‹

Struggling to connect to Ollama from Open WebUI? It could be because Ollama isnâ€™t listening on a network interface that allows external connections. Letâ€™s sort that out:

Configure Ollama to Listen Broadly ğŸ§: Set OLLAMA_HOST to 0.0.0.0 to make Ollama listen on all network interfaces. Update Environment Variables: Ensure that the OLLAMA_HOST is accurately set within your deployment environment. Restart OllamağŸ”„: A restart is needed for the changes to take effect.

Configure Ollama to Listen Broadly ğŸ§: Set OLLAMA_HOST to 0.0.0.0 to make Ollama listen on all network interfaces.

Configure Ollama to Listen Broadly ğŸ§: Set OLLAMA_HOST to 0.0.0.0 to make Ollama listen on all network interfaces.

OLLAMA_HOST

0.0.0.0

Update Environment Variables: Ensure that the OLLAMA_HOST is accurately set within your deployment environment.

Update Environment Variables: Ensure that the OLLAMA_HOST is accurately set within your deployment environment.

OLLAMA_HOST

Restart OllamağŸ”„: A restart is needed for the changes to take effect.

Restart OllamağŸ”„: A restart is needed for the changes to take effect.

ğŸ’¡ After setting up, verify that Ollama is accessible by visiting the WebUI interface.

For more detailed instructions on configuring Ollama, please refer to the Ollama's Official Documentation.

ğŸ³ Docker Connection Errorâ€‹

If you're seeing a connection error when trying to access Ollama, it might be because the WebUI docker container can't talk to the Ollama server running on your host. Letâ€™s fix that:

Adjust the Network Settings ğŸ› ï¸: Use the --network=host flag in your Docker command. This links your container directly to your hostâ€™s network. Change the Port: Remember that the internal port changes from 3000 to 8080.

Adjust the Network Settings ğŸ› ï¸: Use the --network=host flag in your Docker command. This links your container directly to your hostâ€™s network.

Adjust the Network Settings ğŸ› ï¸: Use the --network=host flag in your Docker command. This links your container directly to your hostâ€™s network.

--network=host

Change the Port: Remember that the internal port changes from 3000 to 8080.

Change the Port: Remember that the internal port changes from 3000 to 8080.

Example Docker Command:

docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main

docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main

ğŸ”— After running the above, your WebUI should be available at http://localhost:8080.

http://localhost:8080

ğŸ”’ SSL Connection Issue with Hugging Faceâ€‹

Encountered an SSL error? It could be an issue with the Hugging Face server. Here's what to do:

Check Hugging Face Server Status: Verify if there's a known outage or issue on their end. Switch Endpoint: If Hugging Face is down, switch the endpoint in your Docker command.

Check Hugging Face Server Status: Verify if there's a known outage or issue on their end.

Check Hugging Face Server Status: Verify if there's a known outage or issue on their end.

Switch Endpoint: If Hugging Face is down, switch the endpoint in your Docker command.

Switch Endpoint: If Hugging Face is down, switch the endpoint in your Docker command.

Example Docker Command for Connected Issues:

docker run -d -p 3000:8080 -e HF_ENDPOINT=https://hf-mirror.com/ --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main

docker run -d -p 3000:8080 -e HF_ENDPOINT=https://hf-mirror.com/ --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main

ğŸ Podman on MacOSâ€‹

Running on MacOS with Podman? Hereâ€™s how to ensure connectivity:

Enable Host Loopback: Use --network slirp4netns:allow_host_loopback=true in your command. Set OLLAMA_BASE_URL: Ensure it points to http://host.containers.internal:11434.

Enable Host Loopback: Use --network slirp4netns:allow_host_loopback=true in your command.

Enable Host Loopback: Use --network slirp4netns:allow_host_loopback=true in your command.

--network slirp4netns:allow_host_loopback=true

Set OLLAMA_BASE_URL: Ensure it points to http://host.containers.internal:11434.

Set OLLAMA_BASE_URL: Ensure it points to http://host.containers.internal:11434.

http://host.containers.internal:11434

Example Podman Command:

podman run -d --network slirp4netns:allow_host_loopback=true -p 3000:8080 -e OLLAMA_BASE_URL=http://host.containers.internal:11434 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main

podman run -d --network slirp4netns:allow_host_loopback=true -p 3000:8080 -e OLLAMA_BASE_URL=http://host.containers.internal:11434 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main